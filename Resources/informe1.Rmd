---
title: Agrupación y Clasificación de Usuarios Según Tweets - Recolección y Preparación
  de Datos
author: Alonso, Francisco - Astor, Miguel - Hernandez, Jesús - Machado, Javier - Sisco,
  Yilber
date: "4 de febrero de 2016"
output: pdf_document
---

#Propuesta:

Tomando una muestra de tweets, agrupar a cada usuario dentro de esa muestra según las caracteristicas de los tweets que publica, para luego poder realizar una clasificación de nuevos usuarios basándose en las clases obtenidas.

Agrupar y clasificar en clases a cada usuario ppdría proveer información útil a otros usuarios sobre el contenido que publica un grupo de usuarios en específico, por ejemplo el conjunto de usuarios que sigue su cuenta de Tweeter. Puede proveer también información sobre los intereses de un usuario específico, ésta información puede ser usada con fines de mercadeo, segmentación de publicidad o estudio de las tendencias en una región.

##Objetivos de negocio

Obtener información sobre los usuarios que permita mejorar la toma de decisiones en distintas áreas donde sea comun el uso de Tweeter. 

##Factores que determinan el éxito del proceso

###Precisión y confiabilidad de la clasificación de los usuarios
La calidad de la información obtenida y los objetivos planteados dependen totalmente de estos factores. No es posible diferenciar usuarios y tomar decisiones respecto a ellos si los resultados no son suficientemente precisos y confiables. 

##Fuente de datos

La data necesaria es el conjunto de tweets de cada usuario en la muestra. La recolección de datos será realizada por medio del API que provee Tweeter, desde R se obtendrá una muestra de usuarios y un subconjunto de los tweets de su "TimeLine".

##Objetivos del proceso de minería de datos

- Agrupar a los usuarios y obtener un conjunto de clases que describan las diferencias entre cada usuario. Tarea de MD: Agrupación o Clustering.

- Clasificar a un conjunto de usuarios en base a las clases obtenidas en la fase de agrupación. Tarea de MD: Clasificación.

#Recolección de datos

Para realizar el proceso de recolección es necesario crear una cuenta en Twitter y crear una aplicación mediante la cuál se va a tener acceso al API de Tweeter.

El primer paso realizado fue la recolección aleatoria de un conjunto de tweets en un intervalo de tiempo específico y una región definida por latitud, longitud y un radio. 

```{r eval=FALSE}
  tweets <- searchTwitter(""  # Texto requerido en el tweet
                         , n=10 # Número de tweets solicitados
                         , lang="es" # Idioma Español
                         , since="2015-10-01" # Fecha inicial
                         , geocode='10.480594,-66.903606,7mi'#Ubicación
                         )
```

De esta primera muestra se toman los usuarios:

```{r, eval=FALSE}
  allUsers <- tweets$screenName
  allUsers <- unique(allUsers)
```

  Luego se obtienen 50 tweets por usuario, a cada tweet se le da un formato el cual consiste en remover los caracteres expeciales, los números y se transforman todos los caracteres a minúsculas. 

```{r, eval=FALSE} 

  userTweets = userTimeline(allUsers[i], 50, includeRts = F)
  userTweets = sapply(userTweets, function(x) x$getText())
  userTweets = cleanme(userTweets)
  
  allClean = c(allClean, userTweets)

```

  Luego de tener preparados los tweets de cada usuario se procede a remover las palabras "vacías", éstas no tienen un valor útil dentro del texto ya que son muy comunes y/o no proveen información sobre algún tópico en especial.

```{r, eval=FALSE} 
  allClean = removeWords(allClean, c(stopwords("spanish"))) # remueve palabras vacías
  sw <- readLines("stopwords.es.txt", encoding="UTF-8")
  sw = iconv(sw, to="ASCII//TRANSLIT")
  allClean = removeWords(allClean,sw)
```

  Por último se genera un objeto de tipo Corpus que contiene un conjunto de documentos (conjunto de tweets). Este objeto se formatea de nuevo y se genera a partir de él, una matriz Término Documento aplicando el cálculo de pesos TF-IDF, esta muestra cada término y la cantidad de veces que se encontró ese término entre los tweets de un usuario específico. 

```{r, eval=FALSE} 
  corpus = Corpus(VectorSource(allClean))
  corpus = tm_map(corpus, removeWords, sw) # remueve palabras vacías
  corpus = tm_map(corpus, stripWhitespace) # remove espacios en blanco extras
  corpus <- tm_map(corpus, content_transformer(tolower))
  # matriz término documento
  tdm = TermDocumentMatrix(corpus
                           ,
                           control = list(weighting =
                                            function(x)
                                              weightTfIdf(x, normalize =
                                                            FALSE),
                                          stopwords = TRUE)) 
  tdmMatrix <- as.matrix(tdm)
  write.csv(tdmMatrix, 'TDM.csv') # exporta el data frame a csv
```

  A partir de esta matriz se requiere hacer una limpieza más exhaustiva de términos, la data está muy esparcida con una gran cantidad de valores en cero. Luego se puede proceder con las siguientes iteraciones del proceso de minería de datos. 

##Inconvenenientes encontrados

  Durante el proceso de limpieza se encontraron tweets con una gran cantidad de íconos y "emojis", estos presentan un gran obstáculo al momento de limpiar el conjunto de datos ya que producen muchos errores y no hay herramientas específicas para lidiar con el formato, codificación y caracteres especiales que arrojan al conjunto de datos.

  Las herramientas para lematización requieren más investigación de nuestra parte ya que las encontradas no proveen los resultados mínimos necesarios para usarlas en este proceso. Se probaron la biblioteca "SnowballC" y una aplicación llamada "Grampal".

```{r, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE, results="hide"} 
  
  getTweets <- function()
{
  #install this packages
  library(twitteR)
  library(RCurl)
  library(RJSONIO)
  library(stringr)
  library(tm)
  library(wordcloud)
  library(dplyr)
  library(SnowballC)

  tweetCount = 10
  
  # Declare Twitter API Credentials
  api_key <- "vl7qDRn1ooQbeksFv4RwrBQ1d"#"API KEY" # From dev.twitter.com
  api_secret <- "8O9WrcW6CFSw5K4y2FMlpxTDJWtgRJzj0Zt8aZx5D9rlK9LjMy"#"API Secret" # From dev.twitter.com
  token <- "4815410248-drkzmNLBG3vVmQ9ek0yxmERqO6UVsSD32ZDfsCQ"#"Access token" # From dev.twitter.com
  token_secret <- "J07InbjfqnUkBv1PrkluJrVLUX9Zg51elOZFsFCmc08yc"#"Access token secret" # From dev.twitter.com
  
  # Create Twitter Connection
  setup_twitter_oauth(api_key, api_secret, token, token_secret)
  
  
  # Run Twitter Search. Format is searchTwitter("Search Terms", n=100, lang="en", geocode="lat,lng", also accepts since and until).
  
  tweets <- searchTwitter(""  # text inside the tweet
                         , n=100 # number of tweets to get
                         , lang="es" # Español
                         , since="2015-10-01" # since which date
                         #, until = "2015-12-31" 
                         , geocode='10.480594,-66.903606,7mi') # latitude, longitude, radius
  # Transform tweets list into a data frame
  
  tweets.df <- twListToDF(tweets)
   #show the tweets data frame
  #tweets.users = filter(tweets.df, isRetweet == F) %>% select(text, screenName)
  tweets.users = tweets.df %>% select(text, screenName)
  tweets.users$text <- sapply(tweets.users$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
  
  tweets.users$text  <- sapply(tweets.users$text,function(row) iconv(row, 'UTF-8', 'ASCII'))
  
  #usableText=str_replace_all(tweets$text,"[^[:graph:]]", " ") 
  #print(usableText)
  tweets.users$text <- cleanme(tweets.users$text)
  
#   tweets.users$text = cleanme(antmicros_txt)
  
  allUsers <- tweets.users$screenName
  
  #all = removeWords(as.matrix(tweets.users), c(stopwords("spanish")))
  #sw <- readLines("stopwords.es.txt", encoding="UTF-8")
  #sw = iconv(sw, to="ASCII//TRANSLIT")
  #all = removeWords(all,sw)
  allUsers <- unique(allUsers)
  #print(allUsers)
  #print(length(allUsers))
  
  #df <- data.frame(id=as.integer())
  allClean <- c()
  
  userTweets = userTimeline(allUsers[1], 50, includeRts = F)
  userTweets = sapply(userTweets, function(x) x$getText())
  userTweets = cleanme(userTweets)
  
  allClean = c(allClean, userTweets)
  
  userTweets = userTimeline(allUsers[1], 50, includeRts = F)
  userTweets = sapply(userTweets, function(x) x$getText())
  userTweets = cleanme(userTweets)
  
  allClean = c(allClean, userTweets)
  
  userTweets = removeWords(userTweets, c(stopwords("spanish")))
  sw <- readLines("stopwords.es.txt", encoding="UTF-8")
  sw = iconv(sw, to="ASCII//TRANSLIT")
  userTweets = removeWords(userTweets,sw)
  corpus = Corpus(VectorSource(userTweets))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus = tm_map(corpus, removeWords, sw)
  corpus = tm_map(corpus, stripWhitespace)
  tdm = TermDocumentMatrix(corpus)
  
  m= as.matrix(tdm)
  wf <- sort(rowSums(m),decreasing=TRUE)
  dm <- data.frame(word = names(wf), freq=wf)
  dm <- filter(dm, freq > 2)
  
  
  #for(i in 1:length(allUsers))
  for(i in 2:3)
  {
    userTweets = userTimeline(allUsers[i], 50, includeRts = F)
    userTweets = sapply(userTweets, function(x) x$getText())
    userTweets = cleanme(userTweets)
    
    allClean = c(allClean, userTweets)
    
#     userTweets = removeWords(userTweets, c(stopwords("spanish")))
#     sw <- readLines("stopwords.es.txt", encoding="UTF-8")
#     sw = iconv(sw, to="ASCII//TRANSLIT")
#     userTweets = removeWords(userTweets,sw)
#     corpus = Corpus(VectorSource(userTweets))
#     corpus <- tm_map(corpus, content_transformer(tolower))
#     corpus = tm_map(corpus, removeWords, sw)
#     corpus = tm_map(corpus, stripWhitespace)
#     tdm = TermDocumentMatrix(corpus)
#     
#     m= as.matrix(tdm)
#     wf <- sort(rowSums(m),decreasing=TRUE)
#     dmHelper <- data.frame(word = names(wf), freq=wf)
#     dm <- merge(dm, filter(dmHelper, freq > 2))
#     #View(dmHelper)
  }
  
  all = removeWords(allClean, c(stopwords("spanish")))
  sw <- readLines("stopwords.es.txt", encoding="UTF-8")
  sw = iconv(sw, to="ASCII//TRANSLIT")
  all = removeWords(all,sw)
  corpus = Corpus(VectorSource(all))
  # remueve palabras vacías personalizada
  corpus = tm_map(corpus, removeWords, sw)
  # remove espacios en blanco extras
  corpus = tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, content_transformer(tolower))
  
  tdm = TermDocumentMatrix(corpus,
                           control = list(weighting =
                                            function(x)
                                              weightTfIdf(x, normalize =
                                                            FALSE),
                                          stopwords = TRUE))
  #= as.matrix(tdm)
  #View(m)
  
  #conteo de palabras en orden decreciente
  #wf <- sort(rowSums(m),decreasing=TRUE)
  
  #crea un data frame con las palabras y sus frecuencias
  #dm <- data.frame(word = names(wf), freq=wf) 
  #exporta el data frame a csv
  
  tdmMatrix <- as.matrix(tdm)
  View(tdmMatrix)

  write.csv(tdmMatrix, 'TDM.csv')
  #print(row.names(tdmMatrix)[[1]])
  #stringRows <- sapply(row.names(tdmMatrix), function(w) as.String(w), USE.NAMES = F)
  #keepRows <- sapply(stringRows, function(w) grepl("[[:digit:]]", w), USE.NAMES = F)
  
  #keepRows <- grepl(as.String("+"), levels(factor(row.names(tdmMatrix))))
  #print(as.String(row.names(tdmMatrix)[1]))
  #View(as.data.frame(stringRows))
  #View(as.data.frame(keepRows))
}


cleanme = function(x)
{
  # cambia a minúsculas
  #x = tolower(x)
  # remueve retweets
  x = gsub("(rt|via)((?:\\b\\W*@\\w+)+)", "", x)
  # remueve @
  x = gsub("@\\w+", "", x)
  # remueve links
  x = gsub("(https?:\\/\\/)\\w+(.\\w+)+(.\\w+) ", "", x)
  x = gsub("www.\\w+(.*) ", "", x)
  x = gsub("(https?:\\/\\/)\\w+(.\\w+)+(.\\w+)", "", x)
  x = gsub("www.\\w+(.*)", "", x)
  # caracteres especiales
  x = gsub("<", "", x)
  x = gsub(">", "", x)
  # remueve simbolos de puntuación
  x = gsub("[[:punct:]]", "", x)
  # remove números
  x = gsub("[[:digit:]]", "", x)
  # remueve tabs
  x = gsub("[ |\t]{2,}", "", x)
  # remueve espacio en blanco al inicio
  x = gsub("^ ", "", x)
  # remueve espacio en blanco al final
  x = gsub(" $", "", x)
  return(x)
}

getTweets()
  
  
```









